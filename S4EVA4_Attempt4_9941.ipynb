{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S4EVA4_Attempt4_9941.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajy4683/EVA4S4/blob/master/S4EVA4_Attempt4_9941.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbZiFNg_soz5",
        "colab_type": "text"
      },
      "source": [
        "Final Accuracy: 99.41\n",
        "###  Number of parameters - 10874"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "outputId": "4cc54e6f-8518-4d66-fdf0-ce93facfbbc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auzVE6mdHdzZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "2d50a122-45b2-46fe-cc46-8c1e9d5aa717"
      },
      "source": [
        "!pip install pytorch-ignite\n",
        "!pip install torchsummary\n",
        "!pip install wandb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.4.0)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (0.8.25)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.5.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.6.1)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.0.7)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Requirement already satisfied: gql==0.2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.2.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (4.0.2)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.14.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.10.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: gitdb2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (2.0.6)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: graphql-core<2,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (1.1)\n",
            "Requirement already satisfied: pathtools>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
            "Requirement already satisfied: smmap2>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from gitdb2>=2.0.0->GitPython>=1.0.0->wandb) (2.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXqF5fu1mV8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "from torch.utils.data import DataLoader\n",
        "from ignite.handlers import Checkpoint, DiskSaver\n",
        "\n",
        "\n",
        "import logging\n",
        "logging.propagate = False \n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N8hW64W9CHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.propagate = False \n",
        "logging.getLogger().setLevel(logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWIjKEiV7azI",
        "colab_type": "code",
        "outputId": "0874521c-6ce8-454e-c02e-de273283b10d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import wandb\n",
        "#wandb.init()\n",
        "#!wandb login a6f947d2d2f69e7a8c8ca0f69811fd554f27d204\n",
        "!wandb login a6f947d2d2f69e7a8c8ca0f69811fd554f27d204"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OONup7Ukaulv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, 3, padding=1) #input -? OUtput? RF\n",
        "        self.conv1_bn = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(16, 16, 3, padding=1)\n",
        "        self.conv3_bn = nn.BatchNorm2d(16)\n",
        "        self.conv4 = nn.Conv2d(16, 16, 3, padding=1)\n",
        "        self.conv4_bn = nn.BatchNorm2d(16)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Conv2d(16, 16, 3)\n",
        "        self.conv5_bn = nn.BatchNorm2d(16)\n",
        "        self.conv6 = nn.Conv2d(16, 16, 3)\n",
        "        self.conv6_bn = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(16, 10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "        # x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        # x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        # x = F.relu(self.conv7(x))\n",
        "        # x = x.view(-1, 10)\n",
        "        # return F.log_softmax(x)\n",
        "\n",
        "        x = self.conv1_bn(F.relu(self.conv1(x))) ### Layer 1 ---->(Relu(BN(Conv)))\n",
        "        x = self.pool1(self.conv2_bn(F.relu(self.conv2(x)))) ### Layer 2---> MaxPool((Relu(BN(Conv))))\n",
        "        x = self.conv3_bn(F.relu(self.conv3(x))) ### Layer 3 ---->(Relu(BN(Conv)))\n",
        "        x = self.pool2(self.conv4_bn(F.relu(self.conv4(x)))) ### Layer 4---> MaxPool((Relu(BN(Conv))))\n",
        "        #########\n",
        "        x = self.conv5_bn(F.relu(self.conv5(x))) ### Layer 5---> (Relu(BN(Conv)))        \n",
        "        #x = F.adaptive_avg_pool2d(F.relu(self.conv6_bn(self.conv6(x))),(1,1))### Layer 6 ---> (Relu(BN(Conv))) F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = self.conv6_bn(F.relu(self.conv6(x)))\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        #F.avg_pool2d(x,x.size()[2:])### Layer 6 ---> (Relu(BN(Conv))) F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = self.conv7(x) #### Final Layer (Conv)\n",
        "        x = x.view(-1, 10)\n",
        "\n",
        "\n",
        "        # x = F.relu(self.conv1_bn(self.conv1(x))) ### Layer 1 ---->(Relu(BN(Conv)))\n",
        "        # x = self.pool1(F.relu(self.conv2_bn(self.conv2(x)))) ### Layer 2---> MaxPool((Relu(BN(Conv))))\n",
        "        # x = F.relu(self.conv3_bn(self.conv3(x))) ### Layer 3 ---->(Relu(BN(Conv)))\n",
        "        # x = self.pool2(F.relu(self.conv4_bn(self.conv4(x)))) ### Layer 4---> MaxPool((Relu(BN(Conv))))\n",
        "        # #########\n",
        "        # x = F.relu(self.conv5_bn(self.conv5(x))) ### Layer 5---> (Relu(BN(Conv)))        \n",
        "        # #x = F.adaptive_avg_pool2d(F.relu(self.conv6_bn(self.conv6(x))),(1,1))### Layer 6 ---> (Relu(BN(Conv))) F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        # x = F.relu(self.conv6_bn(self.conv6(x)))\n",
        "        # x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        # #F.avg_pool2d(x,x.size()[2:])### Layer 6 ---> (Relu(BN(Conv))) F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        # x = self.conv7(x) #### Final Layer (Conv)\n",
        "        # x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ediZhP_nRWG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Net(nn.Module):\n",
        "#     def __init__(self,start_channels=32, exponetate_layers=True):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.start_channels = start_channels\n",
        "#         self.multiplier = 2\n",
        "\n",
        "#         if (exponetate_layers == False):\n",
        "#             self.multiplier = 1\n",
        "        \n",
        "#         self.conv1 = nn.Conv2d(1, self.start_channels, 3, padding=1) #input -? OUtput? RF\n",
        "#         self.conv1_bn = nn.BatchNorm2d(self.start_channels)\n",
        "#         self.conv2 = nn.Conv2d(self.start_channels, self.start_channels*self.multiplier, 3, padding=1)\n",
        "#         self.conv2_bn = nn.BatchNorm2d(self.start_channels*self.multiplier)\n",
        "\n",
        "#         self.start_channels = self.start_channels * self.multiplier\n",
        "#         self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "#         self.conv3 = nn.Conv2d(self.start_channels, self.start_channels*self.multiplier, 3, padding=1)\n",
        "#         self.conv3_bn = nn.BatchNorm2d(self.start_channels*self.multiplier)\n",
        "#         #self.start_channels = self.start_channels * self.multiplier\n",
        "#         self.conv4 = nn.Conv2d(self.start_channels, self.start_channels*self.multiplier, 3, padding=1)\n",
        "#         self.conv4_bn = nn.BatchNorm2d(self.start_channels*self.multiplier)\n",
        "#         #self.start_channels = self.start_channels * self.multiplier\n",
        "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "\n",
        "#         # self.conv5 = nn.Conv2d(self.start_channels, self.start_channels*self.multiplier, 3)\n",
        "#         # self.conv5_bn = nn.BatchNorm2d(self.start_channels*self.multiplier)\n",
        "#         # #self.start_channels = self.start_channels * self.multiplier\n",
        "#         # self.conv6 = nn.Conv2d(self.start_channels, self.start_channels*self.multiplier, 3)\n",
        "#         # self.conv6_bn = nn.BatchNorm2d(self.start_channels*self.multiplier)\n",
        "#         # #self.start_channels = self.start_channels * self.multiplier\n",
        "\n",
        "#         # self.conv7 = nn.Conv2d(self.start_channels, 10, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.conv1_bn(self.conv1(x))) ### Layer 1 ---->(Relu(BN(Conv)))\n",
        "#         x = self.pool1(F.relu(self.conv2_bn(self.conv2(x)))) ### Layer 2---> MaxPool((Relu(BN(Conv))))\n",
        "#         x = F.relu(self.conv3_bn(self.conv3(x))) ### Layer 3 ---->(Relu(BN(Conv)))\n",
        "#         x = self.pool2(F.relu(self.conv4_bn(self.conv4(x)))) ### Layer 4---> MaxPool((Relu(BN(Conv))))\n",
        "#         #########\n",
        "#         x = F.relu(self.conv5_bn(self.conv5(x))) ### Layer 5---> (Relu(BN(Conv)))        \n",
        "#         #x = F.adaptive_avg_pool2d(F.relu(self.conv6_bn(self.conv6(x))),(1,1))### Layer 6 ---> (Relu(BN(Conv))) F.adaptive_avg_pool2d(x, (1, 1))\n",
        "#         x = F.relu(self.conv6_bn(self.conv6(x)))\n",
        "#         x = F.avg_pool2d(x,x.size()[2:])### Layer 6 ---> (Relu(BN(Conv))) F.adaptive_avg_pool2d(x, (1, 1))\n",
        "#         x = self.conv7(x) #### Final Layer (Conv)\n",
        "#         x = x.view(-1, 10)\n",
        "#         return F.log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "outputId": "632dc2a8-1d81-414b-be5d-34b319299912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# model = Net(start_channels=8,exponetate_layers=True).to(device)\n",
        "# summary(model, input_size=(1, 28, 28))\n",
        "# #wandb.watch(model, log=\"all\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-47fd511d5c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexponetate_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'start_channels'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU0GFDJNJXWS",
        "colab_type": "code",
        "outputId": "2249dbeb-ed58-4566-ce6e-d67b33b66cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "source": [
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              80\n",
            "       BatchNorm2d-2            [-1, 8, 28, 28]              16\n",
            "            Conv2d-3           [-1, 16, 28, 28]           1,168\n",
            "       BatchNorm2d-4           [-1, 16, 28, 28]              32\n",
            "         MaxPool2d-5           [-1, 16, 14, 14]               0\n",
            "            Conv2d-6           [-1, 16, 14, 14]           2,320\n",
            "       BatchNorm2d-7           [-1, 16, 14, 14]              32\n",
            "            Conv2d-8           [-1, 16, 14, 14]           2,320\n",
            "       BatchNorm2d-9           [-1, 16, 14, 14]              32\n",
            "        MaxPool2d-10             [-1, 16, 7, 7]               0\n",
            "           Conv2d-11             [-1, 16, 5, 5]           2,320\n",
            "      BatchNorm2d-12             [-1, 16, 5, 5]              32\n",
            "           Conv2d-13             [-1, 16, 3, 3]           2,320\n",
            "      BatchNorm2d-14             [-1, 16, 3, 3]              32\n",
            "           Conv2d-15             [-1, 10, 1, 1]             170\n",
            "================================================================\n",
            "Total params: 10,874\n",
            "Trainable params: 10,874\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.42\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.47\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3YKrywtQrSc",
        "colab_type": "code",
        "outputId": "3e281a76-2753-48a5-a1bb-30fb02e88981",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_loader.dataset.data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "\n",
        "from tqdm import tqdm\n",
        "def train(args, model, device, train_loader, optimizer, epoch_number):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    train_loss = 0\n",
        "    train_accuracy = 0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        train_accuracy += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    print('\\nEpoch: {:.0f} Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
        "        epoch_number, train_loss, train_accuracy, len(train_loader.dataset),\n",
        "        100. * train_accuracy / len(train_loader.dataset)))\n",
        "    \n",
        "    wandb.log({        \n",
        "        \"Train Accuracy\": 100. * train_accuracy / len(train_loader.dataset),\n",
        "        \"Train Loss\": train_loss})\n",
        "\n",
        "\n",
        "def test(args, model, device, test_loader,classes,epoch_number):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    example_images = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        #example_images.append(wandb.Image(\n",
        "        #        data[0], caption=\"Pred: {} Truth: {}\".format(classes[pred[0].item()], classes[target[0]])))\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nEpoch: {:.0f} Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
        "        epoch_number, test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    \n",
        "    wandb.log({\n",
        "        \"Test Accuracy\": 100. * correct / len(test_loader.dataset),\n",
        "        \"Test Loss\": test_loss})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R",
        "colab_type": "code",
        "outputId": "7a06e7db-011c-45b8-b202-f9faaf5e5837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#model = Net(start_channels=16,exponetate_layers=False).to(device)\n",
        "wandb.init(project=\"news4eva4\")\n",
        "wandb.watch_called = False # Re-run the model without restarting the runtime, unnecessary after our next release\n",
        "\n",
        "# WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "config = wandb.config          # Initialize config\n",
        "config.batch_size = 64          # input batch size for training (default: 64)\n",
        "config.test_batch_size = 64    # input batch size for testing (default: 1000)\n",
        "config.epochs = 20             # number of epochs to train (default: 10)\n",
        "config.lr = 0.01               # learning rate (default: 0.01)\n",
        "config.momentum = 0.9          # SGD momentum (default: 0.5) \n",
        "config.no_cuda = False         # disables CUDA training\n",
        "config.seed = 1               # random seed (default: 42)\n",
        "config.log_interval = 10     # how many batches to wait before logging training status\n",
        "\n",
        "def main():\n",
        "    use_cuda = not config.no_cuda and torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    \n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    # random.seed(config.seed)       # python random seed\n",
        "    torch.manual_seed(config.seed) # pytorch random seed\n",
        "    # numpy.random.seed(config.seed) # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Load the dataset: We're training our CNN on CIFAR10 (https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "    # First we define the tranformations to apply to our images\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True,\n",
        "                        transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])),\n",
        "        batch_size=config.batch_size, shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))\n",
        "                        ])),\n",
        "        batch_size=config.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "    # Initialize our model, recursively go over all modules and convert their parameters and buffers to CUDA tensors (if device is set to cuda)\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config.lr,\n",
        "                          momentum=config.momentum)\n",
        "    \n",
        "    # WandB – wandb.watch() automatically fetches all layer dimensions, gradients, model parameters and logs them automatically to your dashboard.\n",
        "    # Using log=\"all\" log histograms of parameter values in addition to gradients\n",
        "    wandb.watch(model, log=\"all\")\n",
        "\n",
        "    for epoch in range(1, config.epochs + 1):\n",
        "        train(config, model, device, train_loader, optimizer, epoch)\n",
        "        test(config, model, device, test_loader, classes,epoch)\n",
        "        \n",
        "    # WandB – Save the model checkpoint. This automatically saves a file to the cloud and associates it with the current run.\n",
        "    torch.save(model.state_dict(), \"model.h5\")\n",
        "    wandb.save('model.h5')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# model = Net().to(device)\n",
        "# summary(model, input_size=(1, 28, 28))\n",
        "\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "# wandb.watch(model, log=\"all\")\n",
        "# for epoch in range(1, 20):\n",
        "#     train(model, device, train_loader, optimizer, epoch)\n",
        "#     test(model, device, test_loader)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/rajy4683/news4eva4\" target=\"_blank\">https://app.wandb.ai/rajy4683/news4eva4</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/rajy4683/news4eva4/runs/jxp9b3oo\" target=\"_blank\">https://app.wandb.ai/rajy4683/news4eva4/runs/jxp9b3oo</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Wandb version 0.8.26 is available!  To upgrade, please run:\n",
            "wandb:  $ pip install wandb --upgrade\n",
            "  0%|          | 0/938 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=0.06803488731384277 batch_id=937: 100%|██████████| 938/938 [00:17<00:00, 54.90it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1 Train set: Average loss: 0.0031, Accuracy: 57167/60000 (95.278%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1 Test set: Average loss: 0.0472, Accuracy: 9871/10000 (98.710%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.07141637802124023 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 55.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2 Train set: Average loss: 0.0007, Accuracy: 59201/60000 (98.668%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2 Test set: Average loss: 0.0337, Accuracy: 9884/10000 (98.840%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.006737321615219116 batch_id=937: 100%|██████████| 938/938 [00:17<00:00, 54.64it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3 Train set: Average loss: 0.0005, Accuracy: 59407/60000 (99.012%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3 Test set: Average loss: 0.0289, Accuracy: 9904/10000 (99.040%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0045141130685806274 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 55.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 4 Train set: Average loss: 0.0004, Accuracy: 59492/60000 (99.153%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 4 Test set: Average loss: 0.0331, Accuracy: 9895/10000 (98.950%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.003172367811203003 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 56.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 5 Train set: Average loss: 0.0004, Accuracy: 59578/60000 (99.297%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 5 Test set: Average loss: 0.0292, Accuracy: 9911/10000 (99.110%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.02531956136226654 batch_id=937: 100%|██████████| 938/938 [00:17<00:00, 54.77it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 6 Train set: Average loss: 0.0003, Accuracy: 59620/60000 (99.367%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 6 Test set: Average loss: 0.0280, Accuracy: 9924/10000 (99.240%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.007911637425422668 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 56.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 7 Train set: Average loss: 0.0003, Accuracy: 59661/60000 (99.435%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 7 Test set: Average loss: 0.0238, Accuracy: 9929/10000 (99.290%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.005070194602012634 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 57.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 8 Train set: Average loss: 0.0003, Accuracy: 59674/60000 (99.457%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 8 Test set: Average loss: 0.0204, Accuracy: 9938/10000 (99.380%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0010284781455993652 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 55.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 9 Train set: Average loss: 0.0002, Accuracy: 59741/60000 (99.568%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 9 Test set: Average loss: 0.0201, Accuracy: 9939/10000 (99.390%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.003539830446243286 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 56.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 10 Train set: Average loss: 0.0002, Accuracy: 59775/60000 (99.625%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 10 Test set: Average loss: 0.0217, Accuracy: 9928/10000 (99.280%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.00722794234752655 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 57.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 11 Train set: Average loss: 0.0002, Accuracy: 59799/60000 (99.665%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 11 Test set: Average loss: 0.0237, Accuracy: 9939/10000 (99.390%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0006275177001953125 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 56.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 12 Train set: Average loss: 0.0002, Accuracy: 59815/60000 (99.692%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 12 Test set: Average loss: 0.0218, Accuracy: 9940/10000 (99.400%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.00034987926483154297 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 57.20it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 13 Train set: Average loss: 0.0002, Accuracy: 59808/60000 (99.680%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 13 Test set: Average loss: 0.0197, Accuracy: 9941/10000 (99.410%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.002287060022354126 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 57.79it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 14 Train set: Average loss: 0.0001, Accuracy: 59827/60000 (99.712%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 14 Test set: Average loss: 0.0239, Accuracy: 9927/10000 (99.270%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.008025750517845154 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 58.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 15 Train set: Average loss: 0.0001, Accuracy: 59833/60000 (99.722%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 15 Test set: Average loss: 0.0243, Accuracy: 9925/10000 (99.250%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.00013688206672668457 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 58.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 16 Train set: Average loss: 0.0001, Accuracy: 59859/60000 (99.765%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 16 Test set: Average loss: 0.0237, Accuracy: 9932/10000 (99.320%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0068104565143585205 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 56.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 17 Train set: Average loss: 0.0001, Accuracy: 59883/60000 (99.805%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 17 Test set: Average loss: 0.0219, Accuracy: 9935/10000 (99.350%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0008232593536376953 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 56.24it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 18 Train set: Average loss: 0.0001, Accuracy: 59858/60000 (99.763%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 18 Test set: Average loss: 0.0227, Accuracy: 9930/10000 (99.300%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0012362897396087646 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 56.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 19 Train set: Average loss: 0.0001, Accuracy: 59913/60000 (99.855%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/938 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 19 Test set: Average loss: 0.0259, Accuracy: 9922/10000 (99.220%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0010610222816467285 batch_id=937: 100%|██████████| 938/938 [00:16<00:00, 56.47it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 20 Train set: Average loss: 0.0001, Accuracy: 59884/60000 (99.807%)\n",
            "\n",
            "\n",
            "Epoch: 20 Test set: Average loss: 0.0216, Accuracy: 9928/10000 (99.280%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEcI6R2xJmyO",
        "colab_type": "code",
        "outputId": "cc74b0fe-4f03-4860-8c87-57c03a8b7d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.save(model.state_dict(), \"model.h5\")\n",
        "wandb.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/wandb/run-20200206_101221-nhl168h8/model.h5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbfU2VomBpZ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "990aa2fb-f538-418e-98e4-d52ba9a77be2"
      },
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(src='https://app.wandb.ai/rajy4683/news4eva4?workspace=user-rajy4683',width=700, height=600)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"700\"\n",
              "            height=\"600\"\n",
              "            src=\"https://app.wandb.ai/rajy4683/news4eva4?workspace=user-rajy4683\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f2c827f0940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TRKNQAXPCxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}